import os
import pickle
import platform

import dotenv
from huggingface_hub import InferenceClient
from neo4j import GraphDatabase
import pandas as pd
import streamlit as st
from tqdm import tqdm

from graph_rag import app_text as text
from graph_rag import app
from graph_rag.config import model_params

tqdm.pandas()

VISUALISE = True

if platform.system() == "Linux":  # community cloud runs linux
    # Secrets stored in Streamlit in this case
    LOCAL = False
elif platform.system() == "Windows":
    LOCAL = True
    project_dir = os.path.join(os.pardir)
    dotenv_path = os.path.join(project_dir, '.env')
    dotenv.load_dotenv(dotenv_path)
else:
    LOCAL = True

if LOCAL:
    uri = os.getenv("NEO4J_DESKTOP_URI")
    user = os.getenv("NEO4J_USERNAME")
    pwd = os.getenv("NEO4J_DESKTOP_PASSWORD")
else:
    uri = os.getenv("NEO4J_URI")
    user = os.getenv("NEO4J_USERNAME")
    pwd = os.getenv("NEO4J_PASSWORD")

st.title("High Friction Graph Database Semantic Search")

driver = GraphDatabase.driver(uri=uri, auth=(user, pwd), database="neo4j")

try:
    driver.verify_connectivity()
    NEO4J_CONNECTION = True
    st.success("Connected to Neo4j DB")
except:
    NEO4J_CONNECTION = False
    st.warning("Failed to connect to Neo4j DB. Using pre-populated query results instead. Please check Neo4j connection paramaters and try again")

st.markdown(text.title_text())

st.markdown("## The British National Bibliography")

st.markdown(text.bnb_text())

st.markdown("## Embedding")
emb_i, emb_a1, emb_a2, emb_models = text.embedding_text()
st.markdown(emb_i)

with st.expander("Analogy One: RGB Colour"):
    st.markdown(emb_a1, unsafe_allow_html=True)

with st.expander("Analogy Two: XKCD - Features of Adulthood"):
    st.markdown(emb_a2, unsafe_allow_html=True)

st.markdown(emb_models)

st.dataframe(pd.read_csv("static\\model_params.csv", index_col=0, encoding="utf-8-sig"))

st.markdown("## Search")

holding_query = "What do you want to search for today?"
q = st.text_input(
    "Search box",
    value=holding_query,
    help="The query will be embedded and compared to the embedded subset of the BNB",
    label_visibility="hidden"
)

if q != holding_query:

    with driver.session() as session:
        limit = 5
        kw_result = session.execute_read(app.kw_search, query=q, limit=limit)
        kw_res = ["" for x in range(5)]
        for i, x in enumerate(kw_result):
            kw_res[i] = x[0]

        results_df = pd.DataFrame(
            data={"Keyword Search": kw_res},
            index=pd.RangeIndex(limit)
        )

        for m in model_params.keys():
            embedder = model_params[m]["model"]
            idx = "titleLCSH" + model_params[m]["acronym"].capitalize()
            semantic_result = session.execute_read(app.semantic_search, query=q, model=embedder, idx=idx, nn=limit)
            semantic_res = [(x[0][0], x[1]) for x in semantic_result]
            results_df[m] = [x[0] for x in semantic_res]
            results_df[m + "_uri"] = [x[1] for x in semantic_res]
        st.success("Search complete")

try:
    st.table(results_df[["Keyword Search", "all-MiniLM-L6-v2", "all-mpnet-base-v2", "all-distilroberta-v1"]])
except:
    st.write("Oops that search term failed, please try another.")

st.markdown("## Retrieval Augmented Generation")

st.markdown(text.rag_text())

model = st.radio(
    label="Choose the embedding model output you'd like to summarise with Mistral-7B.",
    options=["all-MiniLM-L6-v2", "all-mpnet-base-v2", "all-distilroberta-v1"]
)

run_rag = st.button("Run RAG")

if run_rag and q == holding_query:
    st.write("Please run a search on the database, then retry.")

elif run_rag and q != holding_query:
    titles = "; ".join(results_df[model].values)
    client = InferenceClient(provider="hf-inference", api_key=os.environ["HF_API_KEY"])
    n_requests = 1
    with st.spinner(text="Get retrieval augmented generation content from HF API"):
        responses = app.rag(client=client, titles=titles, n_requests=n_requests)

    if responses:
        tabs = st.tabs([f"Response {x}" for x in range(1, n_requests + 1)])
        for r, tab in zip(responses, tabs):
            tab.write(f"LLM generated response to semantic search results:")
            tab.markdown(f">{r.choices[0].message['content']}")

st.markdown("## Ethics")
st.markdown(text.ethics_text())

st.markdown("## Visualising Results")

if not VISUALISE:
    st.write("Visualisations toggled off")

if VISUALISE and not LOCAL:
    st.write(text.remote_warning())
    umap_df = pickle.load(open(f"static/{model_params[model]['acronym']}_umap.p", "rb"))
    umap_fig = app.create_umap_fig(umap_df)
    with st.spinner("Creating Plotly chart"):
        st.plotly_chart(umap_fig)

# Requires storage for the embedded files so that they can be umap-ed
if VISUALISE and LOCAL:
    st.write(text.visualise_select())
    vis_select = st.radio(
        label="Do you want to use your search, or searches of the interim catalogue generated by BL users?",
        options=["BL user searches", "My search"]
    )

    # subject headings for plotting
    with st.spinner("Running UMAP"):
        umap_df, reducer = app.create_umap(model)
    umap_fig = app.create_umap_fig(umap_df)

    if vis_select == "My search" and q != holding_query:
        with st.spinner("Adding your search to figure"):
            hovertext_df = umap_df.loc[pd.Index(results_df[model + "_uri"].values), "Title"]

            x, y, z = umap_df.loc[results_df[model + "_uri"].values, ["Feature 1", "Feature 2", "Feature 3"]].values.T
            umap_fig.add_scatter3d(x=x, y=y, z=z, hoverinfo="text", hovertext=hovertext_df, showlegend=False,
                                   mode="markers", marker={"size": 8, "symbol": "cross", "color": "black"})

            embedded_search = model_params[model]["model"].encode(q)

            x, y, z = reducer.transform(embedded_search)
            umap_fig.add_scatter3d(x=x, y=y, z=z, hoverinfo="text", hovertext=f"Search term: {q}", showlegend=False,
                                   mode="markers", marker={"size": 12, "symbol": "cross", "color": "gold"})

    if vis_select == "BL user searches":
        # user queries
        with st.spinner("Adding BL user searches to figure"):
            informational_umap = app.umap_transform_user_search(model, reducer)

            hovertext_df = informational_umap.iloc[:5].loc[:, :"Results"].apply(
                lambda x: f"Search: {x['Search String']}<br>Total searches: {x['Searches']}<br>KW Hits {x['Results']}",
                axis=1)

            x, y, z = informational_umap.iloc[:5][["Feature 1", "Feature 2", "Feature 3"]].values.T
            umap_fig.add_scatter3d(x=x, y=y, z=z, hoverinfo="text", hovertext=hovertext_df, showlegend=False,
                                   mode="markers", marker={"size": 10, "symbol": "cross", "color": "black"})

    st.plotly_chart(umap_fig)
